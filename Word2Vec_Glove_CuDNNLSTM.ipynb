{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Glove_CuDNNLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv446IrSIpsDzT6PJcUq1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Torsha-Sett/Toxic-comment-challenge/blob/main/Word2Vec_Glove_CuDNNLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly1P6RDYyERG",
        "outputId": "0d648f54-6a74-466f-9297-e86499a2caab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Toxic comment challenge/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlYDnmhs0JVw",
        "outputId": "e6ac4f9c-a47a-4c35-a780-4f3f07c7bb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Toxic comment challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "#from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jM6_S1gGz2Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "test = pd.read_csv(\"test.csv.zip\")\n",
        "test_y = pd.read_csv(\"test_labels.csv.zip\")"
      ],
      "metadata": {
        "id": "bx5nmzrN0qbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=0\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "list_sentences_train = train[\"comment_text\"]\n",
        "list_sentences_test = test[\"comment_text\"]"
      ],
      "metadata": {
        "id": "jj6YyhV_05kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
      ],
      "metadata": {
        "id": "5pPdwfv61zoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_JdlOz2s7qOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadEmbeddingMatrix(typeToLoad):\n",
        "        # load different embedding file from Kaggle depending on which embedding \n",
        "        # matrix we are going to experiment with\n",
        "        if(typeToLoad==\"glove\"):\n",
        "            #EMBEDDING_FILE='../input/glove-twitter/glove.twitter.27B.25d.txt'\n",
        "            embeddings_index_ = pd.read_pickle('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl')\n",
        "            embeddings_index = {key:value for key, value in embeddings_index_.items() if len(value)==300}\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "            embed_size = 300\n",
        "        elif(typeToLoad==\"word2vec\"):\n",
        "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "            embed_size = 300\n",
        "        elif(typeToLoad==\"fasttext\"):\n",
        "            EMBEDDING_FILE='../input/fasttext/wiki.simple.vec'\n",
        "            embed_size = 300\n",
        "\n",
        "        if(typeToLoad==\"fasttext\" ):#typeToLoad==\"glove\" or \n",
        "            embeddings_index = dict()\n",
        "            # Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
        "            f = open(EMBEDDING_FILE)\n",
        "            for line in tqdm(f):\n",
        "                try:\n",
        "                    # split up line into an indexed array\n",
        "                    values = line.split()\n",
        "                    # first index is word\n",
        "                    word = values[0]\n",
        "                    # store the rest of the values in the array as a new array\n",
        "                    coefs = np.asarray(values[1:], dtype='float32')\n",
        "                    # ignore values without headers as word\n",
        "                    if len(coefs)==300:\n",
        "                        embeddings_index[word] = coefs #300 dimension\n",
        "                    else:\n",
        "                        continue\n",
        "                except:\n",
        "                    # split up line into an indexed array\n",
        "                    values = line.split()\n",
        "                    # first index is word or sometimes the second as well\n",
        "                    word = values[1]\n",
        "                    # store the rest of the values in the array as a new array\n",
        "                    coefs = np.asarray(values[2:], dtype='float32')\n",
        "                    print(values[:2])\n",
        "                    print(len(coefs))\n",
        "                    embeddings_index[word] = coefs #300 dimension\n",
        "            f.close()\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "            #return embeddings_index\n",
        "        elif typeToLoad==\"word2vec\":\n",
        "            embeddings_index = dict()\n",
        "            for word in word2vecDict.wv.vocab:\n",
        "                embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "            \n",
        "        gc.collect()\n",
        "        # We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
        "        # same statistics for the rest of our own random generated weights. \n",
        "        all_embs = np.stack(embeddings_index.values())\n",
        "        #np.stack(list(embeddings_index.values()))\n",
        "        emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
        "        \n",
        "        nb_words = len(tokenizer.word_index)\n",
        "        # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
        "        # the size will be Number of Words in Vocab X Embedding Size\n",
        "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "        gc.collect()\n",
        "\n",
        "        # With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
        "        # our own dictionary and loaded pretrained embedding. \n",
        "        embeddedCount = 0\n",
        "        for word, i in tokenizer.word_index.items():\n",
        "            i-=1\n",
        "            # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            # and store inside the embedding matrix that we will train later on.\n",
        "            if embedding_vector is not None: \n",
        "                embedding_matrix[i] = embedding_vector\n",
        "                embeddedCount+=1\n",
        "        print('total embedded:',embeddedCount,'common words')\n",
        "        \n",
        "        del(embeddings_index)\n",
        "        gc.collect()\n",
        "        \n",
        "        # finally, return the embedding matrix\n",
        "        return embedding_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "d1HVnF_M2UNw",
        "outputId": "54fe2e02-7961-4ff0-aaa4-c78ee3d14332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8bcbe668771b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_index_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://nlp.stanford.edu/data/glove.twitter.27B.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     ) as handles:\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 681\u001b[0;31m                         \u001b[0;34m\"Multiple files found in ZIP file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m                         \u001b[0;34mf\"Only one file per ZIP: {zip_names}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                     )\n",
            "\u001b[0;31mValueError\u001b[0m: Multiple files found in ZIP file. Only one file per ZIP: ['glove.twitter.27B.25d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.200d.txt']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = loadEmbeddingMatrix('word2vec')"
      ],
      "metadata": {
        "id": "vl4pfWS4EjZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
        "\n",
        "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(inp)\n",
        "\n",
        "#x = Bidirectional(LSTM(60, return_sequences=True, name='lstm_layer', dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "x = Bidirectional(CuDNNLSTM(60, return_sequences=True, name='lstm_layer'))(x)\n",
        "x = Dropout(0.1)(x)\n"
      ],
      "metadata": {
        "id": "G0iPem0-EpP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(6, activation=\"sigmoid\")(x)\n"
      ],
      "metadata": {
        "id": "opgQcq8OFFjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OWqRzI_iFVNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some predictions\n",
        "print(list_sentences_test[10])\n",
        "print(dict(zip(list_classes, model.predict(X_te[10:11])[0])))\n",
        "print()\n",
        "print()\n",
        "print(list_sentences_test[1001])\n",
        "print(dict(zip(list_classes, model.predict(X_te[1001:1002])[0])))\n",
        "print()\n",
        "print()\n",
        "\n",
        "# training data\n",
        "i=2018\n",
        "print(list_sentences_train[i])\n",
        "print(dict(zip(list_classes, model.predict(X_t[i:i+1])[0])))\n",
        "print('Actual Values:', train[list_classes].iloc[i].to_dict())\n",
        "print()\n",
        "print()\n",
        "i=159554\n",
        "print(list_sentences_train[i])\n",
        "print(dict(zip(list_classes, model.predict(X_t[i:i+1])[0])))\n",
        "print('Actual Values:', train[list_classes].iloc[i].to_dict())"
      ],
      "metadata": {
        "id": "ikJj_qMnFWPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochRange = np.arange(1,5,1)\n",
        "plt.plot(epochRange,all_losses['baseline_loss'])\n",
        "plt.plot(epochRange,all_losses['baseline_val_loss'])\n",
        "plt.title('Training Vs Validation loss for baseline model')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pyaCNadmFkZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}